import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# ----------------------------
# ğŸ”§ GerÃ¤teeinrichtung
# ----------------------------
# Bestimmt, ob eine GPU verfÃ¼gbar ist und setzt das entsprechende GerÃ¤t.
# Dies ist wichtig fÃ¼r die Beschleunigung von Trainingsprozessen bei groÃŸen Modellen.
gerÃ¤t = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"âœ… Verwendetes GerÃ¤t: {gerÃ¤t}")


# ----------------------------
# ğŸ“‚ Datenladen
# ----------------------------
def lade_daten(pfad=None):
    """
    LÃ¤dt die historischen SP500-Daten aus einer CSV-Datei.

    Diese Funktion liest die SP500-Historien aus einer CSV-Datei, konvertiert die 'Date'-Spalte
    in Datetime-Objekte, sortiert die Daten chronologisch und setzt den Index zurÃ¼ck.

    Args:
        pfad (str, optional): Der Pfad zur CSV-Datei mit den SP500-Daten.
                               Wenn kein Pfad angegeben wird, wird ein Standardpfad verwendet.

    Returns:
        pd.DataFrame: Ein DataFrame mit den geladenen und sortierten SP500-Daten.

    Raises:
        FileNotFoundError: Wenn die angegebene Datei nicht gefunden wird.
    """
    if pfad is None:
        # Heimverzeichnis des aktuellen Benutzers ermitteln
        home_dir = os.path.expanduser('~')
        # Relativen Pfad zum Datenverzeichnis hinzufÃ¼gen
        pfad = os.path.join(home_dir, 'Unternehmenssoftware', 'shared_resources', 'sp500_data',
                            'SP500_Index_Historical_Data.csv')

    # ÃœberprÃ¼fen, ob die Datei existiert
    if not os.path.exists(pfad):
        raise FileNotFoundError(f"âŒ Die Datei wurde nicht gefunden: {pfad}")

    # Laden der Daten aus der CSV-Datei
    daten = pd.read_csv(pfad, parse_dates=['Date'])

    # Sicherstellen, dass die Daten nach Datum sortiert sind
    daten.sort_values('Date', inplace=True)
    daten.reset_index(drop=True, inplace=True)

    return daten


# ----------------------------
# ğŸ“Š Datenvorbereitung
# ----------------------------
def bereite_daten_vor(daten, spalten=['Open', 'High', 'Low', 'Close', 'Volume'], sequenz_lÃ¤nge=60):
    """
    Bereitet die Daten fÃ¼r das Training und Testen des Modells vor.

    Diese Funktion skaliert die angegebenen Merkmale, erstellt Sequenzen und teilt die Daten
    in Trainings- und Testsets auf.

    Args:
        daten (pd.DataFrame): Das vollstÃ¤ndige DataFrame mit den SP500-Daten.
        spalten (list, optional): Liste der zu skalierenden Merkmale. Standard ist ['Open', 'High', 'Low', 'Close', 'Volume'].
        sequenz_lÃ¤nge (int, optional): Die LÃ¤nge jeder Sequenz (Anzahl der Zeitschritte). Standard ist 60.

    Returns:
        tuple: EnthÃ¤lt die folgenden Elemente:
            - X_train (torch.Tensor): Trainingssequenzen.
            - y_train (torch.Tensor): Trainingslabels.
            - X_test (torch.Tensor): Testsequenzen.
            - y_test (torch.Tensor): Testlabels.
            - scaler (MinMaxScaler): Der verwendete Scaler zur Denormalisierung.
            - dates_train (np.ndarray): Datumsangaben fÃ¼r das Trainingsset.
            - dates_test (np.ndarray): Datumsangaben fÃ¼r das Testset.
    """
    # Initialisieren des MinMaxScaler zur Skalierung der Daten zwischen 0 und 1
    scaler = MinMaxScaler(feature_range=(0, 1))
    skaliert = scaler.fit_transform(daten[spalten].values)

    X, y = [], []
    dates = []
    for i in range(sequenz_lÃ¤nge, len(skaliert)):
        # Eingabesequenz: die vorherigen 'sequenz_lÃ¤nge' Zeitpunkte
        X.append(skaliert[i - sequenz_lÃ¤nge:i])
        # Label: der 'Open'-Preis zum aktuellen Zeitpunkt
        y.append(skaliert[i, 0])  # 0=Open Zielwert (Open-Preis)
        # Datum des Zielwerts
        dates.append(daten['Date'].iloc[i])

    X, y = np.array(X), np.array(y)
    dates = np.array(dates)

    # Daten in Training und Test aufteilen (80% Training, 20% Test)
    training = int(0.8 * len(X))
    X_train, X_test = X[:training], X[training:]
    y_train, y_test = y[:training], y[training:]
    dates_train, dates_test = dates[:training], dates[training:]

    # In Torch-Tensoren umwandeln
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

    return X_train, y_train, X_test, y_test, scaler, dates_train, dates_test


# ----------------------------
# ğŸ¤– Einfaches LSTM-Modell definieren
# ----------------------------
class EinfachesLSTM(nn.Module):
    """
    Einfaches LSTM-Modell fÃ¼r die Vorhersage des 'Open'-Preises des SP500.

    Dieses Modell besteht aus einer LSTM-Schicht gefolgt von einer vollverbundenen Schicht,
    die den finalen Vorhersagewert berechnet.

    Attributes:
        lstm (nn.LSTM): Die LSTM-Schicht zur Verarbeitung der Sequenzdaten.
        fc (nn.Linear): Die vollverbundene Schicht zur Ausgabe der Vorhersage.
    """

    def __init__(self, input_dim=5, hidden_dim=50, output_dim=1, num_layers=2):
        """
        Initialisiert das EinfachesLSTM-Modell.

        Args:
            input_dim (int, optional): Die Anzahl der Eingangsmerkmale (Features) pro Zeitschritt. Standard ist 5.
            hidden_dim (int, optional): Die Anzahl der Neuronen in der LSTM-Schicht. Standard ist 50.
            output_dim (int, optional): Die Anzahl der Ausgangsmerkmale (in diesem Fall 1 fÃ¼r den 'Open'-Preis). Standard ist 1.
            num_layers (int, optional): Die Anzahl der LSTM-Schichten. Mehr Schichten kÃ¶nnen komplexere Muster lernen. Standard ist 2.
        """
        super(EinfachesLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        """
        FÃ¼hrt eine VorwÃ¤rtsdurchfÃ¼hrung durch.

        Diese Methode definiert, wie die Eingabedaten durch das Netzwerk flieÃŸen.

        Args:
            x (torch.Tensor): Die Eingabesequenz mit der Form (Batch, Sequenz, Feature).

        Returns:
            torch.Tensor: Die vorhergesagten 'Open'-Preise mit der Form (Batch, Output_Size).
        """
        out, _ = self.lstm(x)  # DurchlÃ¤uft die LSTM-Schicht
        out = out[:, -1, :]  # Nimmt den letzten Zeitschritt der LSTM-Ausgabe
        out = self.fc(out)  # FÃ¼hrt die Ausgabe durch die vollverbundene Schicht
        return out


# ----------------------------
# ğŸ‹ï¸â€â™‚ï¸ Modell trainieren
# ----------------------------
def trainiere_modell(modell, daten_loader, verlustfunktion, optimierer, gerÃ¤t, epochen=100):
    """
    Trainiert das gegebene Modell mit den bereitgestellten Daten.

    Diese Funktion fÃ¼hrt das Training des Modells Ã¼ber eine festgelegte Anzahl von Epochen durch.
    Sie berechnet den Verlust, fÃ¼hrt die RÃ¼ckwÃ¤rtsdurchfÃ¼hrung durch und aktualisiert die Modellgewichte.

    Args:
        modell (nn.Module): Das zu trainierende Modell.
        daten_loader (DataLoader): Der DataLoader fÃ¼r das Trainingsset.
        verlustfunktion (nn.Module): Die Verlustfunktion zur Berechnung des Fehlers.
        optimierer (torch.optim.Optimizer): Der Optimierer zur Aktualisierung der Modellgewichte.
        gerÃ¤t (torch.device): Das GerÃ¤t (CPU/GPU), auf dem das Training durchgefÃ¼hrt wird.
        epochen (int, optional): Die Anzahl der TrainingsdurchlÃ¤ufe. Standard ist 100.

    Returns:
        list: Eine Liste der durchschnittlichen Verlustwerte pro Epoche.
    """
    verlust_werte = []
    for epoch in range(epochen):
        modell.train()  # Setzt das Modell in den Trainingsmodus (aktiviert Dropout, BatchNorm etc.)
        gesamt_verlust = 0  # Initialisiert den Verlust fÃ¼r die aktuelle Epoche

        # Iteriert Ã¼ber alle Batches im Trainingsloader
        for inputs, labels in daten_loader:
            inputs, labels = inputs.to(gerÃ¤t), labels.to(gerÃ¤t)

            optimierer.zero_grad()  # Gradienten zurÃ¼cksetzen
            vorhersage = modell(inputs)  # Vorhersage berechnen
            verlust = verlustfunktion(vorhersage, labels)  # Verlust berechnen
            verlust.backward()  # RÃ¼ckwÃ¤rtsdurchlauf (Gradienten berechnen)
            optimierer.step()  # Gewichte aktualisieren

            gesamt_verlust += verlust.item() * inputs.size(0)  # Summiert den Verlust Ã¼ber alle Samples

        durchschnitt_verlust = gesamt_verlust / len(daten_loader.dataset)  # Durchschnittlicher Verlust pro Sample
        verlust_werte.append(durchschnitt_verlust)

        # Gibt den Verlust alle 10 Epochen und in der ersten Epoche aus.
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f'Epoche {epoch + 1}/{epochen}, Verlust: {durchschnitt_verlust:.6f}')

    return verlust_werte


# ----------------------------
# ğŸ§ª Modell testen
# ----------------------------
def teste_modell(modell, daten_loader, scaler, gerÃ¤t, dates_test, speichere_csv=False, csv_pfad='vorhersagen.csv'):
    """
    Testet das trainierte Modell mit den Testdaten und visualisiert die Ergebnisse.

    Diese Funktion fÃ¼hrt Vorhersagen auf den Testdaten durch, denormalisiert die Ergebnisse,
    berechnet FehlermaÃŸe und erstellt einen Plot der tatsÃ¤chlichen vs. vorhergesagten Werte.

    Args:
        modell (nn.Module): Das trainierte Modell.
        daten_loader (DataLoader): Der DataLoader fÃ¼r das Testset.
        scaler (MinMaxScaler): Der verwendete Scaler zur Denormalisierung.
        gerÃ¤t (torch.device): Das GerÃ¤t (CPU/GPU), auf dem das Modell ausgefÃ¼hrt wird.
        dates_test (np.ndarray): Die Datumsangaben fÃ¼r das Testset.
        speichere_csv (bool, optional): Gibt an, ob die Vorhersagen in einer CSV-Datei gespeichert werden sollen. Standard ist False.
        csv_pfad (str, optional): Der Pfad zur CSV-Datei, in der die Vorhersagen gespeichert werden sollen. Standard ist 'vorhersagen.csv'.

    Returns:
        tuple: EnthÃ¤lt die folgenden Elemente:
            - vorhersagen_denorm (np.ndarray): Denormalisierte vorhergesagte 'Open'-Preise.
            - tatsÃ¤chliche_denorm (np.ndarray): Denormalisierte tatsÃ¤chliche 'Open'-Preise.
            - alle_dates (np.ndarray): Die entsprechenden Datumsangaben fÃ¼r die Vorhersagen.
    """
    modell.eval()  # Setzt das Modell in den Evaluierungsmodus (deaktiviert Dropout etc.)
    vorhersagen, tatsÃ¤chliche = [], []
    alle_dates = []

    with torch.no_grad():  # Deaktiviert die Gradientenberechnung fÃ¼r effizienteres Testen
        for batch_idx, (inputs, labels) in enumerate(daten_loader):
            inputs, labels = inputs.to(gerÃ¤t), labels.to(gerÃ¤t)
            outputs = modell(inputs)  # Vorhersage berechnen
            vorhersagen.append(outputs.cpu().numpy())
            tatsÃ¤chliche.append(labels.cpu().numpy())

            # Berechne die Start- und Endindizes fÃ¼r die aktuellen Batch-Daten
            start = batch_idx * daten_loader.batch_size
            end = start + inputs.size(0)

            # FÃ¼ge die entsprechenden Datumswerte hinzu
            alle_dates.extend(dates_test[start:end])

    # Konvertiere die Listen in numpy-Arrays
    vorhersagen = np.concatenate(vorhersagen).flatten()
    tatsÃ¤chliche = np.concatenate(tatsÃ¤chliche).flatten()
    alle_dates = np.array(alle_dates)

    # Denormalisieren der 'Open'-Preise (zurÃ¼ckskalieren)
    vorhersagen_denorm = scaler.inverse_transform(
        np.hstack((
            vorhersagen.reshape(-1, 1),  # Vorhersagen in Spalte 0 (Open)
            np.zeros((vorhersagen.shape[0], 4))  # Nullen fÃ¼r die anderen Spalten
        ))
    )[:, 0]  # Erste Spalte auswÃ¤hlen, da dort Open steht

    tatsÃ¤chliche_denorm = scaler.inverse_transform(
        np.hstack((
            tatsÃ¤chliche.reshape(-1, 1),
            np.zeros((tatsÃ¤chliche.shape[0], 4))
        ))
    )[:, 0]

    # ÃœberprÃ¼fen, ob die LÃ¤ngen der Daten Ã¼bereinstimmen
    if not (len(alle_dates) == len(tatsÃ¤chliche_denorm) == len(vorhersagen_denorm)):
        raise ValueError("Die LÃ¤ngen von Datum, tatsÃ¤chlichen Werten und Vorhersagen mÃ¼ssen Ã¼bereinstimmen.")

    # Ergebnisse plotten mit Datum auf der X-Achse
    plt.figure(figsize=(12, 6))
    plt.plot(alle_dates, tatsÃ¤chliche_denorm, label='TatsÃ¤chlicher Preis', alpha=0.7)
    plt.plot(alle_dates, vorhersagen_denorm, label='Vorhergesagter Preis', alpha=0.7)
    plt.xlabel('Datum')
    plt.ylabel('S&P 500 Open Preis')
    plt.title('TatsÃ¤chlicher vs. Vorhergesagter S&P 500 Open Preis')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # FehlermaÃŸe berechnen
    rmse = np.sqrt(np.mean((vorhersagen_denorm - tatsÃ¤chliche_denorm) ** 2))
    mae = np.mean(np.abs(vorhersagen_denorm - tatsÃ¤chliche_denorm))
    mape = np.mean(np.abs((tatsÃ¤chliche_denorm - vorhersagen_denorm) / tatsÃ¤chliche_denorm)) * 100

    print(f'RMSE auf Testdaten: {rmse:.2f}')
    print(f'MAE auf Testdaten: {mae:.2f}')
    print(f'MAPE auf Testdaten: {mape:.2f}%')

    # Optional: Speichern der Vorhersagen und tatsÃ¤chlichen Werte als CSV
    if speichere_csv:
        ergebnisse_df = pd.DataFrame({
            'Datum': alle_dates,
            'TatsÃ¤chlich': tatsÃ¤chliche_denorm,
            'Vorhergesagt': vorhersagen_denorm
        })
        # Sicherstellen, dass das Verzeichnis existiert
        if os.path.dirname(csv_pfad):
            os.makedirs(os.path.dirname(csv_pfad), exist_ok=True)
        ergebnisse_df.to_csv(csv_pfad, index=False)
        print(f'Vorhersagen und tatsÃ¤chliche Werte wurden in {csv_pfad} gespeichert.')

    return vorhersagen_denorm, tatsÃ¤chliche_denorm, alle_dates


# ----------------------------
# ğŸ Hauptfunktion
# ----------------------------
def haupt():
    """
    Hauptfunktion zur AusfÃ¼hrung des gesamten Prozesses.

    Dieser Prozess umfasst:
        1. Laden der Daten.
        2. Vorbereiten der Daten (Skalierung, Sequenzbildung, Aufteilung).
        3. Erstellen des LSTM-Modells.
        4. Festlegen der Hyperparameter.
        5. Erstellen der DataLoader fÃ¼r Training und Test.
        6. Definieren der Verlustfunktion und des Optimierers.
        7. Trainieren des Modells.
        8. Plotten der Trainingsverlustkurve.
        9. Testen des Modells und Visualisieren der Ergebnisse.
    """
    # Schritt 1: Daten laden
    print("ğŸ“¥ Daten werden geladen...")
    daten = lade_daten()
    print("âœ… Daten erfolgreich geladen.")

    # Schritt 2: Daten vorbereiten
    print("ğŸ”§ Daten werden vorbereitet...")
    sequenz_lÃ¤nge = 60  # Anzahl der Zeitschritte pro Sequenz
    X_train, y_train, X_test, y_test, scaler, dates_train, dates_test = bereite_daten_vor(
        daten, sequenz_lÃ¤nge=sequenz_lÃ¤nge
    )
    print("âœ… Daten erfolgreich vorbereitet.")

    # Schritt 3: Modell erstellen
    print("ğŸ¤– Erstelle das LSTM-Modell...")
    modell = EinfachesLSTM()
    modell.to(gerÃ¤t)  # Modell auf das ausgewÃ¤hlte GerÃ¤t verschieben
    print("âœ… Modell erfolgreich erstellt und auf das GerÃ¤t verschoben.")

    # Schritt 4: Hyperparameter festlegen
    epochen = 100
    batch_grÃ¶ÃŸe = 64
    lernrate = 0.001
    print(f"âš™ï¸ Hyperparameter festgelegt: Epochen={epochen}, Batch-GrÃ¶ÃŸe={batch_grÃ¶ÃŸe}, Lernrate={lernrate}")

    # Schritt 5: DataLoader erstellen
    print("ğŸ“¦ Erstelle DataLoader fÃ¼r Trainings- und Testdaten...")
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_grÃ¶ÃŸe, shuffle=True)

    # FÃ¼r Test-DataLoader mÃ¼ssen wir auch die Datumsinformationen berÃ¼cksichtigen
    # Da DataLoader standardmÃ¤ÃŸig nur die Tensoren liefert, verarbeiten wir die Test-Daten separat
    test_dataset = TensorDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=batch_grÃ¶ÃŸe, shuffle=False)
    print("âœ… DataLoader erfolgreich erstellt.")

    # Schritt 6: Verlustfunktion und Optimierer definieren
    verlustfunktion = nn.MSELoss()
    optimierer = torch.optim.Adam(modell.parameters(), lr=lernrate)
    print("ğŸ” Verlustfunktion und Optimierer definiert.")

    # Schritt 7: Modell trainieren
    print("ğŸ‹ï¸â€â™‚ï¸ Starte das Training des Modells...")
    verlust_werte = trainiere_modell(modell, train_loader, verlustfunktion, optimierer, gerÃ¤t, epochen=epochen)
    print("ğŸ Training abgeschlossen.")

    # Schritt 8: Verlustkurve plotten
    print("ğŸ“ˆ Plotten der Trainingsverlustkurve...")
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochen + 1), verlust_werte, label='Training Verlust')
    plt.xlabel('Epoche')
    plt.ylabel('MSE Verlust')
    plt.title('Training Verlust Ã¼ber Epochen')
    plt.legend()
    plt.grid(True)
    plt.show()
    print("âœ… Trainingsverlustkurve erfolgreich geplottet.")

    # Schritt 9: Modell testen
    print("ğŸ”¬ Testen des Modells mit den Testdaten...")
    # Setzen Sie speichere_csv auf True, um die Vorhersagen als CSV zu speichern
    vorhersagen_denorm, tatsÃ¤chliche_denorm, alle_dates = teste_modell(
        modell, test_loader, scaler, gerÃ¤t, dates_test, speichere_csv=True, csv_pfad='vorhersagen.csv'
    )
    print("âœ… Modell erfolgreich getestet und Ergebnisse visualisiert.")


# ----------------------------
# ğŸ Programm starten
# ----------------------------
if __name__ == "__main__":
    """
    Der Einstiegspunkt des Skripts.

    Wenn dieses Skript direkt ausgefÃ¼hrt wird, startet es die Hauptfunktion.
    Dies ermÃ¶glicht es, den Code sowohl als Modul zu importieren als auch direkt auszufÃ¼hren.
    """
    haupt()
