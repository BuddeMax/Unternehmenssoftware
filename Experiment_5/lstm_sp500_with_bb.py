import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates  # Importieren von matplotlib.dates f√ºr Datumsformatierung
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# ----------------------------
# üîß Ger√§teeinrichtung
# ----------------------------
# Bestimmt, ob eine GPU verf√ºgbar ist und setzt das entsprechende Ger√§t.
# Dies ist wichtig f√ºr die Beschleunigung von Trainingsprozessen bei gro√üen Modellen.
ger√§t = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Verwendetes Ger√§t: {ger√§t}")

# ----------------------------
# üìÇ Datenladen
# ----------------------------
def lade_daten(pfad=None):
    """
    L√§dt die historischen SP500-Daten aus einer CSV-Datei.

    Diese Funktion liest die SP500-Historien aus einer CSV-Datei, konvertiert die 'Date'-Spalte
    in Datetime-Objekte, entfernt Zeilen mit fehlenden Bollinger Bands, sortiert die Daten
    chronologisch und setzt den Index zur√ºck.

    Args:
        pfad (str, optional): Der Pfad zur CSV-Datei mit den SP500-Daten.
                               Wenn kein Pfad angegeben wird, wird ein Standardpfad verwendet.

    Returns:
        pd.DataFrame: Ein DataFrame mit den geladenen und sortierten SP500-Daten inklusive Bollinger Bands.

    Raises:
        FileNotFoundError: Wenn die angegebene Datei nicht gefunden wird.
    """
    if pfad is None:
        # Heimverzeichnis des aktuellen Benutzers ermitteln
        home_dir = os.path.expanduser('~')
        # Relativen Pfad zum Datenverzeichnis hinzuf√ºgen
        pfad = os.path.join(home_dir, 'Unternehmenssoftware/shared_resources/sp500_data/BollingerBands/SP500_Index_Historical_Data_with_BB.csv')

    # √úberpr√ºfen, ob die Datei existiert
    if not os.path.exists(pfad):
        raise FileNotFoundError(f"‚ùå Die Datei wurde nicht gefunden: {pfad}")

    # Laden der Daten aus der CSV-Datei und Parsen der 'Date'-Spalte
    daten = pd.read_csv(pfad, parse_dates=['Date'])

    # Entfernen von Zeilen mit fehlenden Bollinger Bands
    daten = daten.dropna(subset=['BB_Middle', 'BB_Upper', 'BB_Lower']).reset_index(drop=True)

    # Sicherstellen, dass die Daten nach Datum sortiert sind
    daten = daten.sort_values('Date').reset_index(drop=True)

    return daten

# ----------------------------
# üìä Datenvorbereitung
# ----------------------------
def bereite_daten_vor(daten,
                      spalten=['Open', 'High', 'Low', 'Close', 'Volume', 'BB_Middle', 'BB_Upper', 'BB_Lower'],
                      sequenz_l√§nge=60):
    """
    Bereitet die Daten f√ºr das Training und Testen des Modells vor.

    Diese Funktion skaliert die angegebenen Merkmale, erstellt Sequenzen und teilt die Daten
    in Trainings- und Testsets auf.

    Args:
        daten (pd.DataFrame): Das vollst√§ndige DataFrame mit den SP500-Daten.
        spalten (list, optional): Liste der zu skalierenden Merkmale. Standard ist ['Open', 'High', 'Low', 'Close', 'Volume', 'BB_Middle', 'BB_Upper', 'BB_Lower'].
        sequenz_l√§nge (int, optional): Die L√§nge jeder Sequenz (Anzahl der Zeitschritte). Standard ist 60.

    Returns:
        tuple: Enth√§lt die folgenden Elemente:
            - X_train (torch.Tensor): Trainingssequenzen.
            - y_train (torch.Tensor): Trainingslabels.
            - X_test (torch.Tensor): Testsequenzen.
            - y_test (torch.Tensor): Testlabels.
            - scaler (MinMaxScaler): Der verwendete Scaler zur Denormalisierung.
            - dates_train (np.ndarray): Datumsangaben f√ºr das Trainingsset.
            - dates_test (np.ndarray): Datumsangaben f√ºr das Testset.
    """
    # Initialisieren des MinMaxScaler zur Skalierung der Daten zwischen 0 und 1
    scaler = MinMaxScaler(feature_range=(0, 1))
    skaliert = scaler.fit_transform(daten[spalten].values)

    X, y = [], []
    dates = []
    for i in range(sequenz_l√§nge, len(skaliert)):
        # Eingabesequenz: die vorherigen 'sequenz_l√§nge' Zeitpunkte
        X.append(skaliert[i - sequenz_l√§nge:i])
        # Label: der 'Open'-Preis zum aktuellen Zeitpunkt
        y.append(skaliert[i, 0])  # 0=Open Zielwert (Open-Preis)
        # Datum des Zielwerts
        dates.append(daten['Date'].iloc[i])

    X, y = np.array(X), np.array(y)
    dates = np.array(dates)

    # Daten in Training und Test aufteilen (80% Training, 20% Test)
    training = int(0.8 * len(X))
    X_train, X_test = X[:training], X[training:]
    y_train, y_test = y[:training], y[training:]
    dates_train, dates_test = dates[:training], dates[training:]

    # In Torch-Tensoren umwandeln
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

    return X_train, y_train, X_test, y_test, scaler, dates_train, dates_test

# ----------------------------
# ü§ñ Einfaches LSTM-Modell definieren
# ----------------------------
class EinfachesLSTM(nn.Module):
    """
    Einfaches LSTM-Modell f√ºr die Vorhersage des 'Open'-Preises des SP500.

    Dieses Modell besteht aus einer LSTM-Schicht gefolgt von einer vollverbundenen Schicht,
    die den finalen Vorhersagewert berechnet.

    Attributes:
        lstm (nn.LSTM): Die LSTM-Schicht zur Verarbeitung der Sequenzdaten.
        fc (nn.Linear): Die vollverbundene Schicht zur Ausgabe der Vorhersage.
    """
    def __init__(self, input_dim=8, hidden_dim=50, output_dim=1, num_layers=2):
        """
        Initialisiert das EinfachesLSTM-Modell.

        Args:
            input_dim (int, optional): Die Anzahl der Eingangsmerkmale (Features) pro Zeitschritt. Standard ist 8.
            hidden_dim (int, optional): Die Anzahl der Neuronen in der LSTM-Schicht. Standard ist 50.
            output_dim (int, optional): Die Anzahl der Ausgangsmerkmale (in diesem Fall 1 f√ºr den 'Open'-Preis). Standard ist 1.
            num_layers (int, optional): Die Anzahl der LSTM-Schichten. Mehr Schichten k√∂nnen komplexere Muster lernen. Standard ist 2.
        """
        super(EinfachesLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        """
        F√ºhrt eine Vorw√§rtsdurchf√ºhrung durch.

        Diese Methode definiert, wie die Eingabedaten durch das Netzwerk flie√üen.

        Args:
            x (torch.Tensor): Die Eingabesequenz mit der Form (Batch, Sequenz, Feature).

        Returns:
            torch.Tensor: Die vorhergesagten 'Open'-Preise mit der Form (Batch, Output_Size).
        """
        out, _ = self.lstm(x)          # Durchl√§uft die LSTM-Schicht
        out = out[:, -1, :]            # Nimmt den letzten Zeitschritt der LSTM-Ausgabe
        out = self.fc(out)             # F√ºhrt die Ausgabe durch die vollverbundene Schicht
        return out

# ----------------------------
# üèãÔ∏è‚Äç‚ôÇÔ∏è Modell trainieren
# ----------------------------
def trainiere_modell(modell, daten_loader, verlustfunktion, optimierer, ger√§t, epochen=100):
    """
    Trainiert das gegebene Modell mit den bereitgestellten Daten.

    Diese Funktion f√ºhrt das Training des Modells √ºber eine festgelegte Anzahl von Epochen durch.
    Sie berechnet den Verlust, f√ºhrt die R√ºckw√§rtsdurchf√ºhrung durch und aktualisiert die Modellgewichte.

    Args:
        modell (nn.Module): Das zu trainierende Modell.
        daten_loader (DataLoader): Der DataLoader f√ºr das Trainingsset.
        verlustfunktion (nn.Module): Die Verlustfunktion zur Berechnung des Fehlers.
        optimierer (torch.optim.Optimizer): Der Optimierer zur Aktualisierung der Modellgewichte.
        ger√§t (torch.device): Das Ger√§t (CPU/GPU), auf dem das Training durchgef√ºhrt wird.
        epochen (int, optional): Die Anzahl der Trainingsdurchl√§ufe. Standard ist 100.

    Returns:
        list: Eine Liste der durchschnittlichen Verlustwerte pro Epoche.
    """
    verlust_werte = []
    for epoch in range(epochen):
        modell.train()         # Setzt das Modell in den Trainingsmodus (aktiviert Dropout, BatchNorm etc.)
        gesamt_verlust = 0     # Initialisiert den Verlust f√ºr die aktuelle Epoche

        # Iteriert √ºber alle Batches im Trainingsloader
        for inputs, labels in daten_loader:
            inputs, labels = inputs.to(ger√§t), labels.to(ger√§t)

            optimierer.zero_grad()         # Gradienten zur√ºcksetzen
            vorhersage = modell(inputs)    # Vorhersage berechnen
            verlust = verlustfunktion(vorhersage, labels)  # Verlust berechnen
            verlust.backward()             # R√ºckw√§rtsdurchlauf (Gradienten berechnen)
            optimierer.step()              # Gewichte aktualisieren

            gesamt_verlust += verlust.item() * inputs.size(0)  # Summiert den Verlust √ºber alle Samples

        durchschnitt_verlust = gesamt_verlust / len(daten_loader.dataset)  # Durchschnittlicher Verlust pro Sample
        verlust_werte.append(durchschnitt_verlust)

        # Gibt den Verlust alle 10 Epochen und in der ersten Epoche aus.
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f'Epoche {epoch + 1}/{epochen}, Verlust: {durchschnitt_verlust:.6f}')

    return verlust_werte

# ----------------------------
# üß™ Modell testen
# ----------------------------
def teste_modell(modell, daten_loader, scaler, ger√§t, dates_test=None):
    """
    Testet das trainierte Modell mit den Testdaten und visualisiert die Ergebnisse.

    Diese Funktion f√ºhrt Vorhersagen auf den Testdaten durch, denormalisiert die Ergebnisse,
    berechnet Fehlerma√üe und erstellt einen Plot der tats√§chlichen vs. vorhergesagten Werte.

    Args:
        modell (nn.Module): Das trainierte Modell.
        daten_loader (DataLoader): Der DataLoader f√ºr das Testset.
        scaler (MinMaxScaler): Der verwendete Scaler zur Denormalisierung.
        ger√§t (torch.device): Das Ger√§t (CPU/GPU), auf dem das Modell ausgef√ºhrt wird.
        dates_test (np.ndarray, optional): Die Datumsangaben f√ºr das Testset.
        speichere_csv (bool, optional): Gibt an, ob die Vorhersagen in einer CSV-Datei gespeichert werden sollen. Standard ist False.
        csv_pfad (str, optional): Der Pfad zur CSV-Datei, in der die Vorhersagen gespeichert werden sollen. Standard ist 'vorhersagen.csv'.

    Returns:
        tuple: Enth√§lt die folgenden Elemente:
            - vorhersagen_denorm (np.ndarray): Denormalisierte vorhergesagte 'Open'-Preise.
            - tats√§chliche_denorm (np.ndarray): Denormalisierte tats√§chliche 'Open'-Preise.
            - differenz (np.ndarray): Differenz zwischen vorhergesagten und tats√§chlichen Preisen.
    """
    modell.eval()  # Setzt das Modell in den Evaluierungsmodus (deaktiviert Dropout etc.)
    vorhersagen, tats√§chliche = [], []
    alle_dates = []

    with torch.no_grad():  # Deaktiviert die Gradientenberechnung f√ºr effizienteres Testen
        for batch_idx, (inputs, labels) in enumerate(daten_loader):
            inputs, labels = inputs.to(ger√§t), labels.to(ger√§t)
            outputs = modell(inputs)  # Vorhersage berechnen
            vorhersagen.append(outputs.cpu().numpy())
            tats√§chliche.append(labels.cpu().numpy())

            # Berechne die Start- und Endindizes f√ºr die aktuellen Batch-Daten
            start = batch_idx * daten_loader.batch_size
            end = start + inputs.size(0)

            # F√ºge die entsprechenden Datumswerte hinzu
            alle_dates.extend(dates_test[start:end])

    # Konvertiere die Listen in numpy-Arrays
    vorhersagen = np.concatenate(vorhersagen).flatten()
    tats√§chliche = np.concatenate(tats√§chliche).flatten()
    alle_dates = np.array(alle_dates)

    # Denormalisieren der 'Open'-Preise (zur√ºckskalieren)
    vorhersagen_denorm = scaler.inverse_transform(
        np.hstack((
            vorhersagen.reshape(-1, 1),          # Vorhersagen in Spalte 0 (Open)
            np.zeros((vorhersagen.shape[0], 7))  # Nullen f√ºr die anderen Spalten
        ))
    )[:, 0]  # Erste Spalte ausw√§hlen, da dort Open steht

    tats√§chliche_denorm = scaler.inverse_transform(
        np.hstack((
            tats√§chliche.reshape(-1, 1),
            np.zeros((tats√§chliche.shape[0], 7))
        ))
    )[:, 0]

    # Differenz berechnen
    differenz = vorhersagen_denorm - tats√§chliche_denorm

    # √úberpr√ºfen, ob die L√§ngen der Daten √ºbereinstimmen
    if not (len(alle_dates) == len(tats√§chliche_denorm) == len(vorhersagen_denorm)):
        raise ValueError("Die L√§ngen von Datum, tats√§chlichen Werten und Vorhersagen m√ºssen √ºbereinstimmen.")

    # Ergebnisse plotten mit Jahren auf der x-Achse
    plt.figure(figsize=(12, 6))
    plt.plot(alle_dates, tats√§chliche_denorm, label='Tats√§chlicher Preis', alpha=0.7)
    plt.plot(alle_dates, vorhersagen_denorm, label='Vorhergesagter Preis', alpha=0.7)
    plt.xlabel('Jahr')
    plt.ylabel('S&P 500 Open Preis')
    plt.title('Tats√§chlicher vs. Vorhergesagter S&P 500 Open Preis')
    plt.legend()

    # Formatieren der x-Achse, um Jahre anzuzeigen
    plt.gca().xaxis.set_major_locator(mdates.YearLocator())  # Setzt Hauptticks jedes Jahr
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Formatiert die Ticks als Jahr

    plt.gcf().autofmt_xdate()  # Automatisches Formatieren der Datumsbeschriftungen f√ºr bessere Lesbarkeit

    plt.show()

    # RMSE berechnen (Root Mean Square Error)
    rmse = np.sqrt(np.mean((vorhersagen_denorm - tats√§chliche_denorm) ** 2))
    print(f'RMSE auf Testdaten: {rmse:.2f}')

    # CSV erstellen
    if dates_test is not None:
        df_result = pd.DataFrame({
            'Datum': alle_dates,
            'Tats√§chlicher_Open': tats√§chliche_denorm,
            'Vorhergesagter_Open': vorhersagen_denorm,
            'Differenz': differenz
        })
    else:
        df_result = pd.DataFrame({
            'Tats√§chlicher_Open': tats√§chliche_denorm,
            'Vorhergesagter_Open': vorhersagen_denorm,
            'Differenz': differenz
        })

    # CSV speichern
    csv_pfad = 'lstm_sp500_bb.csv'
    if os.path.dirname(csv_pfad):
        os.makedirs(os.path.dirname(csv_pfad), exist_ok=True)
    df_result.to_csv(csv_pfad, index=False)
    print(f'Die Vorhersagen und tats√§chlichen Werte wurden in {csv_pfad} gespeichert.')

    return vorhersagen_denorm, tats√§chliche_denorm, differenz

# ----------------------------
# üèÅ Hauptfunktion
# ----------------------------
def haupt():
    """
    Hauptfunktion zur Ausf√ºhrung des gesamten Prozesses.

    Dieser Prozess umfasst:
        1. Laden der Daten.
        2. Vorbereiten der Daten (Skalierung, Sequenzbildung, Aufteilung).
        3. Erstellen des LSTM-Modells.
        4. Festlegen der Hyperparameter.
        5. Erstellen der DataLoader f√ºr Training und Test.
        6. Definieren der Verlustfunktion und des Optimierers.
        7. Trainieren des Modells.
        8. Plotten der Trainingsverlustkurve.
        9. Testen des Modells und Visualisieren der Ergebnisse.
    """
    # Schritt 1: Daten laden
    print("üì• Daten werden geladen...")
    daten = lade_daten()
    print("‚úÖ Daten erfolgreich geladen.")

    # Schritt 2: Daten vorbereiten
    print("üîß Daten werden vorbereitet...")
    sequenz_l√§nge = 60  # Anzahl der Zeitschritte pro Sequenz
    X_train, y_train, X_test, y_test, scaler, dates_train, dates_test = bereite_daten_vor(
        daten, sequenz_l√§nge=sequenz_l√§nge
    )
    print("‚úÖ Daten erfolgreich vorbereitet.")

    # Schritt 3: Modell erstellen
    print("ü§ñ Erstelle das LSTM-Modell...")
    modell = EinfachesLSTM()
    modell.to(ger√§t)  # Modell auf das ausgew√§hlte Ger√§t verschieben
    print("‚úÖ Modell erfolgreich erstellt und auf das Ger√§t verschoben.")

    # Schritt 4: Hyperparameter festlegen
    epochen = 100
    batch_gr√∂√üe = 64
    lernrate = 0.001
    print(f"‚öôÔ∏è Hyperparameter festgelegt: Epochen={epochen}, Batch-Gr√∂√üe={batch_gr√∂√üe}, Lernrate={lernrate}")

    # Schritt 5: DataLoader erstellen
    print("üì¶ Erstelle DataLoader f√ºr Trainings- und Testdaten...")
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=batch_gr√∂√üe, shuffle=True)

    test_dataset = TensorDataset(X_test, y_test)
    test_loader = DataLoader(test_dataset, batch_size=batch_gr√∂√üe, shuffle=False)
    print("‚úÖ DataLoader erfolgreich erstellt.")

    # Schritt 6: Verlustfunktion und Optimierer definieren
    verlustfunktion = nn.MSELoss()
    optimierer = torch.optim.Adam(modell.parameters(), lr=lernrate)
    print("üîç Verlustfunktion und Optimierer definiert.")

    # Schritt 7: Modell trainieren
    print("üèãÔ∏è‚Äç‚ôÇÔ∏è Starte das Training des Modells...")
    verlust_werte = trainiere_modell(modell, train_loader, verlustfunktion, optimierer, ger√§t, epochen=epochen)
    print("üèÅ Training abgeschlossen.")

    # Schritt 8: Verlustkurve plotten
    print("üìà Plotten der Trainingsverlustkurve...")
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochen + 1), verlust_werte, label='Training Verlust')
    plt.xlabel('Epoche')
    plt.ylabel('MSE Verlust')
    plt.title('Training Verlust √ºber Epochen')
    plt.legend()
    plt.grid(True)
    plt.show()
    print("‚úÖ Trainingsverlustkurve erfolgreich geplottet.")

    # Schritt 9: Modell testen
    print("üî¨ Testen des Modells mit den Testdaten...")
    vorhersagen_denorm, tats√§chliche_denorm, differenz = teste_modell(
        modell, test_loader, scaler, ger√§t, dates_test=dates_test
    )
    print("‚úÖ Modell erfolgreich getestet und Ergebnisse visualisiert.")

# ----------------------------
# üèÅ Programm starten
# ----------------------------
if __name__ == "__main__":
    """
    Der Einstiegspunkt des Skripts.

    Wenn dieses Skript direkt ausgef√ºhrt wird, startet es die Hauptfunktion.
    Dies erm√∂glicht es, den Code sowohl als Modul zu importieren als auch direkt auszuf√ºhren.
    """
    haupt()
